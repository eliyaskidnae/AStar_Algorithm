{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICS_7lr8xWh4"
      },
      "source": [
        "<img src=\"https://github.com/Fortuz/edu_Adaptive/blob/main/practices/assets/logo.png?raw=1\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HniNUfKTxWh5"
      },
      "source": [
        "Made by **Balázs Nagy** and **Márk Domokos**\n",
        "\n",
        "[<img src=\"https://github.com/Fortuz/edu_Adaptive/blob/main/practices/assets/open_button.png?raw=1\">](https://colab.research.google.com/github/Fortuz/edu_Adaptive/blob/main/practices/L07%20Neural%20Networks%20Train_solved.ipynb)\n",
        "\n",
        "# Labor 07 - Training of a neural Network\n",
        "\n",
        "### Handwritten numbers III.\n",
        "In this exercise we will train our neural network to recognise handwritten digits from 0 to 9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GZwNqkGxWh5"
      },
      "source": [
        "### 1: Imports and Load Data\n",
        "Import the package you want to use and read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U73jQrfxWh5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf15DLk4xWh6"
      },
      "source": [
        "### 2: Data load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q41vkpFxxWh6"
      },
      "source": [
        "The data will be loaded from a publicly available file. An alternative solution would be to upload the data file directly to the google colab file system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4qAyVV-xWh6"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab07/Lab7data.mat\n",
        "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab07/Lab7weights.mat\n",
        "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab07/nnC_history_800.txt\n",
        "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab07/w1_final.txt\n",
        "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab07/w2_final.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovlAPuoMxWh6"
      },
      "source": [
        "Load in the data! Use the Pandas package to do this and then convert it into a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRlvAt3bxWh6"
      },
      "outputs": [],
      "source": [
        "data = loadmat(\"Lab7data.mat\")\n",
        "X = data[\"X\"]\n",
        "y = data [\"y\"]\n",
        "del data\n",
        "m = X.shape[0]\n",
        "print('Shape of X and y in order:')\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "data = loadmat(\"Lab7weights.mat\")\n",
        "w1 = np.array(data[\"Theta1\"])\n",
        "w2 = np.array(data[\"Theta2\"])\n",
        "print('Shape of w1 and w2 in order:')\n",
        "print(w1.shape)\n",
        "print(w2.shape)\n",
        "del data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO9wr4KDxWh7"
      },
      "source": [
        "Consideration in connection with the data:\n",
        "- The samples are sorted into a 5000x400 matrix, which means that we have 5000 20x20 pixel images. The images are stored expanded in the matrix.\n",
        "- As an output, we expect a number, which are stored in a 5000x1 vector.\n",
        "- The weight matrices required for the network to work are 25x401 and 10x26 respectively. This means that our initial input parameter space of 400 is augmented with the BIAS and the hidden layer contains 25 neurons.\n",
        "- The BIAS is also appended to the hidden layer and thus creates the connection between the second weight vector of 26 hidden neurons and the 10 possible outputs.\n",
        "\n",
        "In terms of the operation of the neural network, we will consider the probability of an element belonging to one of the 10 classes, of which we will choose the largest (MaxPooling) to determine the final number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVuC52AaxWh7"
      },
      "source": [
        "### 3: Visualization\n",
        "\n",
        "Visualize the date for a better understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb8puKMBxWh7"
      },
      "outputs": [],
      "source": [
        "print(\"Printing some random data ...\")\n",
        "fig, ax = plt.subplots(10,10, figsize =(8,8))\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        ax[i,j].imshow(X[np.random.randint(0,m+1),:].reshape(20,20, order = \"F\"), cmap=\"hot\")\n",
        "        ax[i,j].axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEwgu3uaxWh7"
      },
      "source": [
        "### 4. Mathematical background\n",
        "\n",
        "For a better understanding, consider the simplified example below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF9I9fe-xWh7"
      },
      "source": [
        "<img src=\"https://github.com/Fortuz/edu_Adaptive/blob/main/practices/assets/Lab07/Pics/L07_Network.png?raw=1\" width=\"550\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDvofkthxWh7"
      },
      "source": [
        "The outlined neural network consists of 3 layers.\n",
        "- Input layer containing 2 input neurons plus the added BIAS.\n",
        "- Hidden layer containing 3 neurons plus the assigned BIAS.\n",
        "- Output layer containing 3 neurons, the maximum of which is selected to get the actual output.\n",
        "\n",
        "In terms of the role of neurons, the treatment of BIAS neurons is slightly different. Since BIAS is not a variable, but a constant value of 1, so that the associated weight can be an independent parameter. Accordingly, it only has a role in weight injection and can be assigned to each layer as required.\n",
        "\n",
        "In terms of indexes introduce the following: <br>\n",
        "$a^{(j)}_{i}$ - activation of the $i^{th}$ neuron in the $j^{th}$ layer <br>\n",
        "$s^{(j)}_{i}$ - summed output of the $i^{th}$ neuron in the $j^{th}$ layer <br>\n",
        "$w^{(j)}_{lk}$ - weight between the $l^{th}$ neuron in the $j^{th}$ layer and the $k^{th}$ neuron in the $j+1^{th}$ layer  <br>\n",
        "$x^{(m)}_{n}$ - the $n^{th}$ feauture in the $m^{th}$ input. ($x^{(1)}_0 = 1)$ is the BIAS.<br>\n",
        "$\\hat y$ - the output\n",
        "\n",
        "The forward step can therefore be structured as follows: <br>\n",
        "The BIAS is added to the $x^{(1)}$ vector and multiplied by the first weight matrix.\n",
        "\n",
        "$ \\underset{1\\times 3}{\\mathrm{x^{(1)}}} \\times \\underset{3\\times 3}{\\mathrm{w^{(1)}}} = \\underset{1\\times 3}{\\mathrm{s^{(2)}}} $\n",
        "\n",
        "We perform the activation in the neurons of the hidden layer. We use the sigmoid function as activation function.\n",
        "\n",
        "$ \\underset{\\color{red}{1\\times 3}}{\\mathrm{a^{(2)}}} = f(\\underset{1\\times 3}{\\mathrm{s^{(2)}}}) = sigmoid(\\underset{1\\times 3}{\\mathrm{s^{(2)}}}) $\n",
        "\n",
        "We assign the BIAS to the hidden layer after the activation, but before the weight is applied!\n",
        "\n",
        "$ \\underset{\\color{red}{1\\times 4}}{\\mathrm{a^{(2)}}} \\times \\underset{4\\times 3}{\\mathrm{w^{(2)}}} = \\underset{1\\times 3}{\\mathrm{s^{(3)}}}  $\n",
        "\n",
        "After activation the value of the neurons in the output layer is obtained.\n",
        "\n",
        "$ \\underset{1\\times 3}{\\mathrm{a^{(3)}}} = f(\\underset{1\\times 3}{\\mathrm{s^{(3)}}}) = sigmoid(\\underset{1\\times 3}{\\mathrm{s^{(3)}}}) $\n",
        "\n",
        "The output layer contains the predictions.\n",
        "\n",
        "$ \\underset{1\\times 3}{\\mathrm{\\hat{y}}} = \\underset{1\\times 3}{\\mathrm{a^{(3)}}} $\n",
        "\n",
        "In a classification problem, the final prediction is obtained by selecting the maximum, i.e. the most likely group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk3YAkCyxWh7"
      },
      "source": [
        "#### Cost function\n",
        "\n",
        "We use the well-established MSE (Mean Square Error) method as a cost function. Let's see how the error calculation is for an output neuron. Ignoring the BIASs, let us focus only on the connections shown in dark color in the figure. <br>\n",
        "\n",
        "$ C = \\sum\\{\\frac{1}{2}(y-\\color{red}{\\hat y})^2 \\} $\n",
        "\n",
        "$ C = \\sum\\{\\frac{1}{2}(y-{\\color{red}{a^{(3)}}})^2 \\} $\n",
        "\n",
        "$ C = \\sum\\{\\frac{1}{2}(y-f({\\color{red}{s^{(3)}}}))^2\\} $\n",
        "\n",
        "$ C = \\sum\\{\\frac{1}{2}(y-f({\\color{red}{a^{(2)}}}w^{(2)}))^2\\} $\n",
        "\n",
        "$ C = \\sum\\{\\frac{1}{2}(y-f(f({\\color{red}{s^{(2)}}})w^{(2)}))^2\\}$\n",
        "\n",
        "$ C = \\sum\\{\\frac{1}{2}(y-f(f(xw^{(1)})w^{(2)}))^2\\} $\n",
        "\n",
        "The error calculation can therefore be reduced to a function of the input variables and the weights in the mesh. The formula can be applied to any number of layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMnYEdG1xWh7"
      },
      "source": [
        "#### Back Propagation\n",
        "\n",
        "The error back propagation is used to calculate the extent to which each neuron is responsible for the error, starting from the output layer and working backwards. In practice, we need to perform half of the steps used to calculate the cost function backwards, which means we need the derivative of the activation function. <br>\n",
        "\n",
        "##### Sigmoid function and its derivative\n",
        "\n",
        "$g(z) = \\frac{1}{1+e^{-z}}$ <br>\n",
        "$\n",
        "\\begin{split}\n",
        "g'(z) =\n",
        "& = \\frac{d}{dz}\\frac{1}{1+e^{-z}} \\\\\n",
        "& = \\frac{1}{(1+e^{-z})^2}(e^{(-z)}) \\\\\n",
        "& = \\frac{1}{1+e^{-z}}(1-\\frac{1}{(1+e^{-z})}) \\\\\n",
        "& = g(z)(1-g(z))\n",
        "\\end{split}\n",
        "$\n",
        "\n",
        "<img src=\"https://github.com/Fortuz/edu_Adaptive/blob/main/practices/assets/Lab07/Pics/L07_SigmoidDeriv.png?raw=1\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osUSThjZxWh7"
      },
      "source": [
        "Back propagation of the error is used to derive the error rate in the neurons of the hidden layer by parsimoniously deriving the cost function according to the elements of the corresponding weight matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wElQxLVhxWh8"
      },
      "source": [
        "$ \\begin{split}\n",
        "\\frac{\\partial C}{\\partial w^{(2)}} = \\frac{\\partial \\sum \\frac{1}{2}(y-\\hat{y})^2}{\\partial w^{(2)}} = \\sum (\\frac{\\partial \\frac{1}{2}(y-\\hat{y})^2}{\\partial w^{(2)}})\n",
        "\\end{split}$\n",
        "\n",
        "For the sake of clarity, let's derive the derivation for one element. <br>\n",
        "\n",
        "$\n",
        "\\begin{split}\n",
        "\\frac{\\partial \\frac{1}{2}(y-\\hat{y})^2}{\\partial w^{(2)}}\n",
        "& = (y-\\hat{y})(-\\frac{\\hat{y}}{\\partial w^{(2)}}) \\\\\n",
        "& = -(y-\\hat{y}) \\cdot \\frac{\\partial \\hat{y}}{\\partial s^{(3)}}\\cdot \\frac{\\partial \ts^{(3)}}{\\partial w^{(2)}}\\\\\n",
        "& = \\color{red}{-(y-\\hat{y}) \\cdot f'(s^{(3)})}\\cdot\n",
        "\\frac{\\partial a^{(2)}w^{(2)}}{\\partial w^{(2)}}\\\\\n",
        "& = {\\color{red} {\\delta^{(3)}}\\cdot a^{(2)}}\n",
        "\\end{split}\n",
        "$<br>\n",
        "\n",
        "Introduce $ \\delta^{(j)}_{i} $ as the error term assigned to neuron i of layer j. <br>\n",
        "Extended in matrix form, after checking the dimensions, we obtain the following relation: <br>\n",
        "$ (a^{(2)})^T \\delta^{(3)} $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSvrOPwgxWh8"
      },
      "source": [
        "#### Training\n",
        "\n",
        "During the teaching phase, we modify the weights using the Gradient Descent method as previously described. To modify the weights, we define a learning rate $(\\mu)$ and optionally specify a penalty term. <br>\n",
        "\n",
        "$ w^{(1)} = w^{(1)} - \\mu \\frac{\\partial C}{w^{(1)}}+ regularization $\n",
        "\n",
        "$ w^{(2)} = w^{(2)} - \\mu \\frac{\\partial C}{w^{(2)}}+ regularization $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq8IFJBAxWh8"
      },
      "source": [
        "#### Algorithm steps\n",
        "\n",
        "Let's summarise the steps to implement the algorithm\n",
        "\n",
        "1, $\\hspace{2mm} xw^{(1)} = s^{(2)} $\n",
        "\n",
        "2, $\\hspace{14mm} f(s^{(2)}) = a^{(2)}$\n",
        "\n",
        "3, $\\hspace{32mm} a^{(2)} w^{(2)} = s^{(3)} $\n",
        "\n",
        "4, $\\hspace{48mm} f(s^{(3)}) = \\hat{y} $\n",
        "\n",
        "5, $\\hspace{2mm} C = \\sum\\{\\frac{1}{2}(y-\\hat y)^2 \\} $\n",
        "\n",
        "6, $\\hspace{2mm} -(y-\\hat{y}) \\cdot f'(s^{(3)}) = \\delta^{(3)} $\n",
        "\n",
        "7, $\\hspace{28mm} (a^{(2)})^T \\delta^{(3)} =  \\frac{\\partial C}{\\partial w^{(2)}} $\n",
        "\n",
        "8, $\\hspace{39mm} \\delta ^{(3)} \\cdot (w^{(2)})^T \\cdot f'(s^{(2)})= \\delta ^{(2)} $\n",
        "\n",
        "9, $\\hspace{78mm} x^T \\delta^{(2)} =  \\frac{\\partial C}{\\partial w^{(1)}} $\n",
        "\n",
        "10, $\\hspace{2mm} w^{(1)} = w^{(1)} - \\mu \\frac{\\partial C}{w^{(1)}}+ regularization $\n",
        "\n",
        "$\\hspace{8mm}w^{(2)} = w^{(2)} - \\mu \\frac{\\partial C}{w^{(2)}}+ regularization $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFQLh96vxWh8"
      },
      "source": [
        "### 5: Parameter preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2CKo0O7xWh8"
      },
      "outputs": [],
      "source": [
        "Lambda = 1                                              # Level of penalty\n",
        "input_layer_size = 400                                  # Input layer size\n",
        "hidden_layer_size = 25                                  # Hidden layer size\n",
        "num_labels = 10                                         # Number of Labels\n",
        "nn_params = np.append(w1.flatten(), w2.flatten())       # Flatten and concatanete w1 and w2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29x_dK6VxWh8"
      },
      "source": [
        "Tracing the architecture is more transparent if the neuron numbers of each layer are stored without the BIAS members. And the weights are stored in an expanded variable. The matrix sizes can be reset based on the neuron numbers of each layer. We will also need the learning rate as a preset parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdnSUJVIxWh8"
      },
      "source": [
        "### 6: Cost function\n",
        "\n",
        "Before implementing the cost function, we need an activation function and its derivative. Again, we choose the sigmoid function as the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8Al1YekxWh8"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    ################### CODE HERE ########################\n",
        "    # Sigmoid function implemnetation\n",
        "\n",
        "\n",
        "    ######################################################\n",
        "    return g\n",
        "\n",
        "def sigmoidGradient(z):\n",
        "    ################### CODE HERE ########################\n",
        "    # Implement the derivative of the sigmoid function as well\n",
        "\n",
        "\n",
        "    ######################################################\n",
        "    return g_deriv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72h-l7HdxWh8"
      },
      "source": [
        "The cost function will receive as parameters the weight parameters, the layer sizes of the mesh, as well as the samples, the labels associated with the samples and the learning rate. Since the weight parameters are passed to the function in a variable, we need to return the corresponding matrix dimensions for the weight matrices within the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oz22ojGxWh8"
      },
      "source": [
        "##### One Hot Encoding\n",
        "\n",
        "To adapt the neural network to the output, we will need to do some more tweaking. The labels of the samples are given by a number, but we are working on a 10-group classification problem. Accordingly, we will have 10 output neurons, which will give us the probability that a given sample belongs to a given class. We will therefore determine the predicted class based on the maximum likelihood. And for this transformation we will use the One Hot Encoding method (only one 1). The essence of this method is to transform each label into a vector with the same number of elements as the number of classes and only one 1 in the column of the corresponding class.\n",
        "\n",
        "Our original labels are the numbers 0 to 9 (left), and the corresponding transformed labels are the vectors of our transformed labels (right). Note that putting the vectors of the transformed labels together is very similar to a unit matrix.\n",
        "\n",
        "<img src=\"https://github.com/Fortuz/edu_Adaptive/blob/main/practices/assets/Lab07/Pics/L07_OneHot.png?raw=1\" width=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE2zltRzxWh8"
      },
      "outputs": [],
      "source": [
        "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda):\n",
        "    # convert nn_params back to w1 and w2 matrices\n",
        "    w1 = nn_params[:((input_layer_size + 1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size + 1)\n",
        "    w2 = nn_params[((input_layer_size + 1) * hidden_layer_size):].reshape(num_labels, hidden_layer_size + 1)\n",
        "\n",
        "    m = X.shape[0]\n",
        "    C = 0\n",
        "    w1_grad = np.zeros((w1.shape))\n",
        "    w2_grad = np.zeros((w2.shape))\n",
        "\n",
        "    ################### CODE HERE ########################\n",
        "    # Forward Step implementation\n",
        "    # Add BIAS to every layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ######################################################\n",
        "\n",
        "    ################### CODE HERE ########################\n",
        "    # ONE HOT ENCODING (y to Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ######################################################\n",
        "\n",
        "    ################### CODE HERE ########################\n",
        "    # PenaltyPenalized cost function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ######################################################\n",
        "\n",
        "    ################### CODE HERE ########################\n",
        "    # Back Propagation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ######################################################\n",
        "    grad = np.append(w1_grad.flatten(), w2_grad.flatten())\n",
        "\n",
        "    return C, grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFl1UIO4xWh9"
      },
      "source": [
        "Let's check if the implemented function works as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKfkuykMxWh9"
      },
      "outputs": [],
      "source": [
        "Lambda = 0\n",
        "C, grad = nnCostFunction(nn_params,input_layer_size,hidden_layer_size,num_labels,X,y,Lambda)\n",
        "print(\"Cost at debugging parameters: %.6f \\nFor Lambda = 0 it should be:  0.287629\" % C)\n",
        "Lambda = 3\n",
        "C, grad = nnCostFunction(nn_params,input_layer_size,hidden_layer_size,num_labels,X,y,Lambda)\n",
        "print(\"Cost at debugging parameters: %.6f \\nFor Lambda = 3 it should be:  0.576051\" % C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-gGJy07xWh9"
      },
      "source": [
        "### 7: Initial weights\n",
        "\n",
        "The initial weights are initialised with a uniform distribution over a defined interval. The weights thus start from a nonzero initial value, which allows convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIT_-EDPxWh9"
      },
      "outputs": [],
      "source": [
        "def randInitializeWeights(L_in, L_out):\n",
        "    epsilon_init = 0.12\n",
        "    W = np.random.rand(L_out,L_in+1)*(2*epsilon_init)-epsilon_init\n",
        "    return W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr9_Ff_yxWh9"
      },
      "outputs": [],
      "source": [
        "initial_w1 = randInitializeWeights(input_layer_size,hidden_layer_size)\n",
        "initial_w2 = randInitializeWeights(hidden_layer_size,num_labels)\n",
        "initial_nn_params = np.append(initial_w1.flatten(),initial_w2.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze1Z6KEGxWh9"
      },
      "source": [
        "### 8: Gradient Descent\n",
        "\n",
        "To train the neural network implement the gradient descent method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx_F4YklxWh9"
      },
      "outputs": [],
      "source": [
        "def gradientDescentnn(X, y, initial_nn_params, lr_rate, num_iters, Lambda, input_layer_size, hidden_layer_size,\n",
        "                      num_labels):\n",
        "\n",
        "    w1 = initial_nn_params[:((input_layer_size + 1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size + 1)\n",
        "    w2 = initial_nn_params[((input_layer_size + 1) * hidden_layer_size):].reshape(num_labels, hidden_layer_size + 1)\n",
        "\n",
        "    m = len(y)\n",
        "    C_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        if (i%20==0):\n",
        "            print('Iteration:', i+1)\n",
        "        elif (i==num_iters-1):\n",
        "            print('Done!')\n",
        "\n",
        "        nn_params = np.append(w1.flatten(), w2.flatten())\n",
        "        C, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda)\n",
        "\n",
        "        w1_grad = grad[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
        "        w2_grad = grad[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)\n",
        "\n",
        "        w1 = w1 - (lr_rate * w1_grad)\n",
        "        w2 = w2 - (lr_rate * w2_grad)\n",
        "        C_history.append(C)\n",
        "\n",
        "    nn_paramsFinal = np.append(w1.flatten(), w2.flatten())\n",
        "    return nn_paramsFinal, C_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gthlVjVxWh9"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "lr_rate = 0.9\n",
        "num_iter = 400\n",
        "Lambda = 0.1\n",
        "\n",
        "nnw, nnC_history = gradientDescentnn(X,y,initial_nn_params,lr_rate,num_iter,Lambda,input_layer_size,hidden_layer_size,num_labels)\n",
        "w1 = nnw[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
        "w2 = nnw[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQPN_e8xWh9"
      },
      "source": [
        "You can experiment with the parameters of the run (Lambda, learning rate, number of iterations).\n",
        "\n",
        "To test the success of a given learning, implement a predictor function that performs the Forward Step and returns the result to its original form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3wbACQtxWh9"
      },
      "outputs": [],
      "source": [
        "def predict(w1,w2,X):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    ################### CODE HERE ########################\n",
        "    # Implement the Forward step\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ######################################################\n",
        "\n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZRozvoBxWiA"
      },
      "source": [
        "Due to the small number of samples, the accuracy of the network is tested on the training data, i.e. how many of the total number of samples were correctly hit. The data are used as a basis to give the accuracy of the network as a percentage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7IR0i08xWiA"
      },
      "outputs": [],
      "source": [
        "def accuracy(pred,y):\n",
        "    acc = (np.mean(pred[:,np.newaxis]==y))*100\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp0IeP-dxWiA"
      },
      "outputs": [],
      "source": [
        "pred = predict(w1,w2,X)\n",
        "acc = accuracy(pred,y)\n",
        "\n",
        "print('Training Set Accuracy after %.0f iteration (currently trained weights): %.2f %%' % (num_iter,acc))\n",
        "\n",
        "w1_r = np.loadtxt(\"w1_final.txt\")\n",
        "w2_r = np.loadtxt(\"w2_final.txt\")\n",
        "\n",
        "pred_800 = predict(w1_r,w2_r,X)\n",
        "acc_800 = accuracy(pred_800,y)\n",
        "print('Training Set Accuracy after 800 iteration (loaded weights): %.2f %%' % acc_800)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--WnmoKAxWiA"
      },
      "source": [
        "### 9: Run training\n",
        "\n",
        "We examine the learning process by plotting the cost function per interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjWyOE6mxWiA"
      },
      "outputs": [],
      "source": [
        "plt.plot(nnC_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost function')\n",
        "plt.title('Cost function over the iterations (Current params)')\n",
        "plt.show()\n",
        "\n",
        "nnC_history_load=np.loadtxt(\"nnC_history_800.txt\")\n",
        "plt.plot(nnC_history_load)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost function')\n",
        "plt.title('Cost function over the iterations (Loaded params)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfj1hRuIxWiA"
      },
      "source": [
        "## Implementation with high level packages\n",
        "\n",
        "With Keras (or Tensorflow) packages, the procedures described in detail above can be implemented as simpler and more compact code. The functions are optimized. Let's see how the example above can be implemented more concisely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOB9TScRxWiA"
      },
      "outputs": [],
      "source": [
        "# imports for array-handling and plotting\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# keras imports for the dataset and building our neural network\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkv-mnqJxWiA"
      },
      "source": [
        "Create a model. Define layers, neuron numbers and activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT_4Gju7xWiA"
      },
      "outputs": [],
      "source": [
        "# building a linear stack of layers with the sequential model\n",
        "model = Sequential()\n",
        "model.add(Dense(400, input_shape=(400,)))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.add(Dense(25))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phAcKOojxWiB"
      },
      "source": [
        "Define cost function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoVYD8NZxWiB"
      },
      "outputs": [],
      "source": [
        "# compiling the sequential model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTIihObpxWiB"
      },
      "source": [
        "Starting from the original data, the One Hot recoding is also required here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cQTZg_vxWiB"
      },
      "outputs": [],
      "source": [
        "Y = np.zeros((m, num_labels))\n",
        "I = np.eye(num_labels)\n",
        "\n",
        "for i in range(1, m+1):\n",
        "    Y[i-1, :] = I[y[i-1]-1, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTkHW90MxWiB"
      },
      "source": [
        "Train the neural network and extract metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0491RRdmxWiB"
      },
      "outputs": [],
      "source": [
        "# training the model and saving metrics in history\n",
        "history = model.fit(X, Y, epochs=20, verbose = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH9Gv9OnxWiB"
      },
      "source": [
        "Checking the execution of a training using the extracted metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF-nSXbUxWiB"
      },
      "outputs": [],
      "source": [
        "# plotting the metrics\n",
        "fig = plt.figure()\n",
        "plt.plot(history.history['acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X6JLCjpxWiB"
      },
      "source": [
        "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}